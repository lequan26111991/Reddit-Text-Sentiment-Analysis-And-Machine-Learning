---
title: "REDDIT SENTIMENT PROJECT"
author: "Richard More | Quan Le | Hao Nguyen | Anirudh Chaudhary"
date: "Date: 02/26/2020"
output:
  html_document:
    theme: readable
    highlight: zenburn
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
header-includes:
  - \hypersetup{colorlinks=true, linkcolor=blue}
  - \usepackage{sectsty}
---

\allsectionsfont{\centering}
\subsectionfont{\raggedright}
\subsubsectionfont{\raggedleft}

<style type="text/css">

h1.title {
  font-size: 35px;
  color: DarkRed;
  text-align: center;
}
h4.author {
    font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkRed;
  text-align: center;
}
h4.date {
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
  text-align: center;
}

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### I. INTRODUCTION

#### A. Overview

<p>
We choose <https://www.reddit.com/> as our primary platform for the sentiment analysis. We chose Corona virus as our subreddit due to the current outbreak. We would like to explore what is the Reddit community's main reaction to this topic.
</p>

#### B. Data Source

<p>
We will use RedditExtractoR, tidyverse, and other relevant libraries. RedditExtractoR (or Reddit Data Extraction Toolkit is created by Ivan Rivera) is the package with which we can fetch the thread and content through the Reddit URL. For more information, please visit <https://www.reddit.com/dev/api>.
</p>

#### C. Summary

<p>
Comments, Comment's scores, and post scores are the attributes we will work with. After cleaning the stopwords and joinging the tokens with  Bing, NRC, and Afinn, we analyzed the trend of Corona Virus through the distribution of Sentiments and comments about Corona and World news over time.
</p>

#### D. Modeling

<p>
Based on these analysis, we try to predict the comment scores with Gradient Boosting Machines and Linear Regression with the H2O instance.
</p>

#### E. Libraries

<p>
The following libraries will be used in our projects
</p>

```{r libs, warning=FALSE, message=FALSE}
library(RedditExtractoR)
library(kableExtra)
library(tidyverse)
library(wordcloud)
library(ggthemes)
library(tidytext)
library(recipes)
library(skimr)
library(h2o)
library(RCurl)
```

### II. REDDIT SENTIMENT ANALYSIS

#### A. Retrieve Reddit Information

```{r gather_data, warning=FALSE, message=FALSE, results="hide"}
r <- reddit_urls(subreddit = "Coronavirus", page_threshold = 20)
rc <- reddit_content(r$URL)
```

```{r data_summary_1, warning=FALSE, message=FALSE}
summary.df <- tibble(Columns = integer(), Rows = integer())
summary.df <- add_row(summary.df, Columns = length(rc), Rows = nrow(rc))

summary.df %>%
  kable() %>%
  add_header_above(header = c("Data summary" = length(summary.df)),
                   bold = TRUE,
                   font_size = "larger",
                   align = "left") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

We have downloaded 20 pages of Reddit post's comments, generating more than 10 thousand observations to work with. The data has 18 columns, out of which we will use only 3, comment, comment_score, and post_score.

```{r extract_data, warning=FALSE, message=FALSE}
rc_rel <- rc[c("comment", "comment_score", "post_score")]
rc_rel <- mutate(rc_rel, id = rownames(rc_rel))

rc_rel[, c(4, 3, 2, 1)] %>%
  head(n = 5) %>%
  kable() %>%
  add_header_above(header = c("Initial look at the data" = length(rc_rel)),
                   bold = TRUE,
                   font_size = "larger",
                   align = "left") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

#### B. Clean and Transform Sentiment

```{r claning}
tokens <- rc_rel %>%
  unnest_tokens(output = word, input = comment)

sw = get_stopwords()
cleaned_tokens <- tokens %>%
  filter(!word %in% sw$word)

nums <- cleaned_tokens %>%
  filter(str_detect(word, "^[0-9]|http[s]?|.com$")) %>%
  select(word) %>% unique()

cleaned_tokens <- cleaned_tokens %>%
  filter(!word %in% nums$word)

rare <- cleaned_tokens %>%
  count(word) %>%
  filter(n < 10) %>%
  select(word) %>%
  unique()

cleaned_tokens <- cleaned_tokens %>%
  filter(!word %in% rare$word)
```

#### C. WordCloud Visualization

Wordcloud all cleaned tokens

```{r wordcloud}
pal <- brewer.pal(8,"Dark2")
cleaned_tokens %>%
  count(word) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors = pal))
```

As we can see the main words are: people, virus, coronavirus, china, cases, etc. We can also find other words related to a virus: health, pandemic, infected, symptoms, spread.

Interestingly we can also determine which countries the virus has already spread: Italy, Korea, Iran, and of course China. The word "us" is not clear without context.

Join tokens with Bing, NRC, and Afinn

```{r sentiments, warning=FALSE, message=FALSE}
sent_reviews = cleaned_tokens %>%
  left_join(get_sentiments("nrc")) %>%
  rename(nrc = sentiment) %>%
  left_join(get_sentiments("bing")) %>%
  rename(bing = sentiment) %>%
  left_join(get_sentiments("afinn")) %>%
  rename(afinn = value)
```

#### D. Sentiment Visualization

Sentiment for the word "corona" is not present in the above mentioned 3 sentiment lexicons, so we have to add our own.

```{r corona_sentiment}
sent_reviews <- sent_reviews %>%
  mutate(bing = ifelse(word == "corona", "negative", bing)) %>%
  mutate(nrc = ifelse(word == "corona", "negative", nrc))
```

##### Bing Lexicon visualization

```{r sentiment_bing}
bing_word_counts <- sent_reviews %>%
  filter(!is.na(bing)) %>%
  count(word, bing, sort = TRUE)

bing_word_counts %>%
  filter(n > 350) %>%
  mutate(n = ifelse(bing == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = bing)) +
    geom_col() +
    coord_flip() +
    labs(y = "Contribution to sentiment") +
    theme_economist()
```

##### NRC Lexicon visualization

```{r sentiment_nrc_wordcloud}
nrc_word_counts <- sent_reviews %>%
  filter(!is.na(nrc)) %>%
  count(word, nrc, sort = TRUE)

nrc_word_counts %>%
  filter(n > 250) %>%
  mutate(n = ifelse(nrc %in%
                      c("negative", "fear", "anger", "sadness", "disgust"),
                    -n,
                    n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = nrc)) +
    geom_col() +
    coord_flip() +
    labs(y = "Contribution to sentiment") +
    theme_economist()
```

##### Distribution of sentiments

```{r sentiment_nrc_graph}
sent_reviews %>%
  filter(!is.na(nrc)) %>%
  group_by(nrc) %>%
  summarise(count = n()) %>%
  ggplot(., aes(x = reorder(nrc, -count), y = count)) +
    geom_bar(stat = "identity") +
    xlab("NRC") +
    ylab("Count") +
    labs(title = "Distribution of sentiments") +
    theme_economist()
```

#### E. Chart for Concern of Corona Virus Overtime on Reddit

In this section we are analyzing the trend about the Corona virus.

The following 2 code chunks are not run when creating the final HTML, but are used to create the CSV for aggregation.

Set up the code to aggregate number of comment about World News and Corona everyday

```{r csv_creation, eval=FALSE}
df_corona <- rc %>%
  filter(str_detect(title,"corona|Corona|CORONA|2019-nCOV") |
         str_detect(title,"COVID-19|Covid-19|covid-19")) %>%
  select(comm_date) %>%
  group_by(comm_date) %>%
  count(comm_date)

colnames(df_corona) <- c("Date", "Count_Corona")

df_world <- rc %>%
  select(comm_date) %>%
  group_by(comm_date) %>%
  count(comm_date)

colnames(df_world) <- c("Date", "Count_World")

df_export <- inner_join(df_world, df_corona, by = "Date")

if (file.exists("corona_over_time.csv") == TRUE){
  write.table(df_export,"corona_over_time.csv",
              append = TRUE, col.names = FALSE, sep = ",", row.names = FALSE)
} else {
  write.table(df_export,"corona_over_time.csv",
              append = FALSE, col.names = TRUE, sep = ",", row.names = FALSE)
}
```

Code to create a daily schedule for Corona aggregation

```{r scheduler, eval=FALSE}
library(taskscheduleR)
myscript <- system.file("reddit_corona.R")

taskscheduler_create(taskname = "CORONA_UPDATE",
                     rscript = "C:/Users/JNK4QBK/Desktop/reddit_corona.R",
                     schedule = "DAILY",
                     starttime = "23:50",
                     startdate = format(Sys.Date() + 1, "%d/%m/%Y"))
```

Import csv of Corona aggregation and visualize the results from the past several days

```{r csv_part}
urlfile="https://raw.githubusercontent.com/hnguyen154/RedditBigDataProject/master/corona_over_time.csv"

x <- getURL(urlfile)
df_corona_plot <- read.csv(text = x)

df_plot <- df_corona_plot %>%
  gather("Count_Corona", "Count_World", key = "Categories", value = "Total_Comments")

ggplot(df_plot, aes(x = Date, y = Total_Comments, fill = Categories)) +
  geom_bar(position = "dodge", stat = "identity") +
  geom_line(aes(x=Date, y=Total_Comments, group=Categories)) +
  geom_point() +
  ylab("Number of comments") +
  labs(title = "Corona virus comment trend") +
  theme_economist() +
  theme(axis.text.x = element_text(angle = 90, hjust = 0, vjust = 0.5))
```

### III. REDDIT SENTIMENT MACHINE LEARNING

For the machine learning part we are trying to create a model that can predict a comment's score based on the score of the parent post and the comment's NRC sentiments (more specifically the distribution of the sentiment types, like 8 trust words and 2 anger words, etc.)

#### A. Preprocess Data for Machine Learning

In the first step we are pulling the sentiment results together for the individual comments for machine learning.

```{r create_df0}
sent_reviews <- sent_reviews %>%
  mutate(id = as.integer(id))

df0 <- sent_reviews %>%
  select("id", "post_score", "comment_score") %>%
  unique()

nrcs <- sent_reviews %>%
  select("nrc") %>%
  unique() %>%
  na.omit() %>%
  as.list()

for (nrcx in nrcs$nrc) {
  df1 <- sent_reviews %>%
    select("id", "nrc") %>%
    filter(nrc == nrcx) %>%
    group_by(id) %>%
    summarise(num = n()) %>%
    arrange(id)

  colnames(df1)[2] <- paste0("nrc_", nrcx)

  df0 <- full_join(df0, df1, by = "id")
}

df0 %>%
  head(n = 5) %>%
  kable() %>%
  add_header_above(header = c("Data for Machine Learning" = length(df0)),
                   bold = TRUE,
                   font_size = "larger",
                   align = "left") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

summary.df <- tibble(Columns = integer(), Rows = integer())
summary.df <- add_row(summary.df, Columns = length(df0), Rows = nrow(df0))

summary.df %>%
  kable() %>%
  add_header_above(header = c("Data summary" = length(summary.df)),
                   bold = TRUE,
                   font_size = "larger",
                   align = "left") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

After combining the id, post_score, and comment_score with the NRC sentiment counts we got 13 columns and the original amount of rows to work with.

Preparing the data for machine learning.

```{r preprocess_df0}
x_train_tbl <- df0 %>% select(-c(comment_score, id))
y_train_tbl <- df0 %>% select(comment_score)

x_train_tbl_skim = partition(skim(x_train_tbl))

rec_obj <- recipe(~ ., data = x_train_tbl) %>%
  step_meanimpute(all_numeric()) %>%
  prep(training = x_train_tbl)

x_train_processed_tbl <- bake(rec_obj, x_train_tbl)
```

#### B. Create Machine Learning Model

Initiate an H2O Intance

```{r h2o_init, warning=FALSE, message=FALSE, results="hide"}
h2o.init(nthreads = -1)
h2o.removeAll()

data_h2o <- as.h2o(
  bind_cols(y_train_tbl, x_train_processed_tbl),
  destination_frame= "train.hex"
)
```

Train, Test, Validate Split in 70%, 15%, and 15%.

```{r data_split}
splits <- h2o.splitFrame(data = data_h2o,
                         ratios = c(0.7, 0.15),
                         seed = 1234)
train_h2o <- splits[[1]]
valid_h2o <- splits[[2]]
test_h2o  <- splits[[3]]
```

#### C. Gradient Boosting Model

```{r h2o_gbm, warning=FALSE, message=FALSE, results="hide"}
y <- "comment_score"
x <- setdiff(names(train_h2o), y)

hyper_prms <- list(
  ntrees = c(45, 50, 55),
  learn_rate = c(0.1, 0.2),
  max_depth = c(20, 25)
)

grid_gbm <- h2o.grid(
  grid_id="grid_gbm",
  algorithm = "gbm",
  training_frame = train_h2o,
  validation_frame = valid_h2o,
  x = x,
  y = y,
  sample_rate = 0.7,
  col_sample_rate = 0.7,
  stopping_rounds = 2,
  stopping_metric = "RMSE",
  stopping_tolerance = 0.0001,
  score_each_iteration = T,
  model_id = "gbm_covType3",
  balance_classes = T,
  seed = 5000,
  hyper_params = hyper_prms
)
```

Get the best model and the metrics for that model from the grid

```{r h2o_results}
gbm_grid <- h2o.getGrid("grid_gbm", sort_by = "rmse", decreasing = FALSE)
gbm_best_model <- h2o.getModel(gbm_grid@summary_table$model_ids[1])

gbm_metrics.df <- tibble(metric = character(), train = double(), valid = double())
gbm_metrics.df <- add_row(gbm_metrics.df,
                      metric = "R^2",
                      train = h2o.r2(gbm_best_model, train = TRUE),
                      valid = h2o.r2(gbm_best_model, valid = TRUE)
                      )
gbm_metrics.df <- add_row(gbm_metrics.df,
                      metric = "MSE",
                      train = h2o.mse(gbm_best_model, train = TRUE),
                      valid = h2o.mse(gbm_best_model, valid = TRUE)
                      )
gbm_metrics.df <- add_row(gbm_metrics.df,
                      metric = "RMSE",
                      train = h2o.rmse(gbm_best_model, train = TRUE),
                      valid = h2o.rmse(gbm_best_model, valid = TRUE)
                      )
gbm_metrics.df <- add_row(gbm_metrics.df,
                      metric = "Mean Residual Deviance",
                      train = h2o.mean_residual_deviance(gbm_best_model, train = TRUE),
                      valid = h2o.mean_residual_deviance(gbm_best_model, valid = TRUE)
                      )

gbm_metrics.df %>%
  kable() %>%
  add_header_above(header = c("Metrics" = length(gbm_metrics.df)),
                   bold = TRUE,
                   font_size = "larger") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

#### D. Linear Regression Model

```{r h2o_lr, warning=FALSE, message=FALSE, results="hide"}
hyper_params <- list(
  alpha = c(0, .25, .5, .75, .1)
)

grid <- h2o.grid(
  grid_id = "grid_lr",
  algorithm = "glm",
  training_frame = train_h2o,
  validation_frame = valid_h2o,
  x = x,
  y = y,
  hyper_params = hyper_params,
  search_criteria = list(strategy = "Cartesian")
)
```

Get the best model and the metrics for that model from the grid

```{r h2o_lr_results}
lr_grid <- h2o.getGrid("grid_lr", sort_by = "rmse", decreasing = FALSE)
lr_best_model <- h2o.getModel(lr_grid@summary_table$model_ids[1])

lr_metrics.df <- tibble(metric = character(), train = double(), valid = double())
lr_metrics.df <- add_row(lr_metrics.df,
                      metric = "R^2",
                      train = h2o.r2(lr_best_model, train = TRUE),
                      valid = h2o.r2(lr_best_model, valid = TRUE)
                      )
lr_metrics.df <- add_row(lr_metrics.df,
                      metric = "MSE",
                      train = h2o.mse(lr_best_model, train = TRUE),
                      valid = h2o.mse(lr_best_model, valid = TRUE)
                      )
lr_metrics.df <- add_row(lr_metrics.df,
                      metric = "RMSE",
                      train = h2o.rmse(lr_best_model, train = TRUE),
                      valid = h2o.rmse(lr_best_model, valid = TRUE)
                      )
lr_metrics.df <- add_row(lr_metrics.df,
                      metric = "Mean Residual Deviance",
                      train = h2o.mean_residual_deviance(lr_best_model, train = TRUE),
                      valid = h2o.mean_residual_deviance(lr_best_model, valid = TRUE)
                      )

lr_metrics.df %>%
  kable() %>%
  add_header_above(header = c("Metrics" = length(lr_metrics.df)),
                   bold = TRUE,
                   font_size = "larger") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```


#### E. Result and Discussion

After we fit the model Linear Regression and Gradient Boosting model, we attempt to find the best model based on the RMSE in the report on validation data. The results show low R-Square and high RMSE for both GBM and Linear Model.

